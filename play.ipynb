{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fddde40",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "from model import GPT, GPTConfig\n",
    "\n",
    "ckpt_path = \"model_19072.pt\"  # adjust if needed\n",
    "\n",
    "# If you hit the PyTorch 2.6 \"weights_only\" safety error, enable the allowlist:\n",
    "# from torch.serialization import safe_globals\n",
    "# with safe_globals({'GPTConfig': GPTConfig}):\n",
    "#     checkpoint = torch.load(ckpt_path, map_location='cpu')\n",
    "# Otherwise, this is fine if you trust your own checkpoint:\n",
    "checkpoint = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n",
    "\n",
    "# Rebuild model from saved config (important: your train.py used vocab_size=50304)\n",
    "config = checkpoint['config']          # this is your GPTConfig dataclass\n",
    "model = GPT(config)\n",
    "\n",
    "# Load weights. Missing attn.bias buffers are OK (they're re-created on init).\n",
    "state_dict = checkpoint['model']\n",
    "# (Optional) If you had saved a wrapped DDP model by mistake, strip prefixes:\n",
    "# state_dict = {k.replace('module.', '', 1) if k.startswith('module.') else k: v\n",
    "#               for k, v in state_dict.items()}\n",
    "missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
    "print(\"missing:\", missing)       # expect things like *.attn.bias (buffers)\n",
    "print(\"unexpected:\", unexpected) # usually []\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device).eval()\n",
    "\n",
    "# -------- simple text generation (sampling) ----------\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(model, prompt, max_new_tokens=100, temperature=1.0, top_k=None, device=None):\n",
    "    device = device or next(model.parameters()).device\n",
    "    ids = torch.tensor(enc.encode(prompt), dtype=torch.long, device=device)[None, ...]  # (1, T)\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits, _ = model(ids)               # (1, T, vocab)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < v[:, [-1]]] = -float('inf')\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)  # (1, 1)\n",
    "        ids = torch.cat([ids, next_id], dim=1)\n",
    "    return enc.decode(ids[0].tolist())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
